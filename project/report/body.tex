\section{Body}

% Explain the paper, in your own words. Don't go into as many details as the original text, but the person reading your review should have a general understanding of the paper's results and how those results can be obtained. The structure and content of this section of course heavily depends on the paper itself. Don't hesitate to split it in multiple sections or subsections, for example:
% \subsection{An algorithm for whatever problem we try to solve}
% If your paper contains theorems, sketch the proofs of important theorems.

% \subsection{Benchmarks}
% If it contains benchmarks, show the key scores or results.

% You can follow the structure of the paper you're reviewing, but write with your own words.

\subsection{An algorithm for whatever problem we try to solve}

At the interface of any security-sensitive function call, there usually exists
a \emph{contract} that describes which inputs or outpus are deemed public
(i.e., known to attackers) and which ones are private (i.e., to protect).
A leak should never reveal anyting only about the private inputs and outputs.
If we denote a program as $p$ (with the usual definition of a prgram being a
sequence of statements or programs) with a corresponding state $s$ (which
is mapping from variables to values) then at each step of execution, from
state $s$ executing program/statement $p$ we can define leakage $L(.)$ as follows:

\begin{equation}
    \begin{split}
    L(\langle s, \text{\texttt{if}}~e~\text{\texttt{then}}~p_1~\text{\texttt{else}}~p_2 \rangle) = & s(e) \\
    L(\langle s, \text{\texttt{while}}~e~\text{\texttt{do}}~p \rangle) = & s(e) \\
    L(\langle s, x_0[e_0] = e \rangle) = & s(e_0)s(e_1)...s(e_n)
    \end{split}
\end{equation}

The first two leakage sources correspond to leakage from control flow while the
latter is leakage from memory access. Note that in the third leakage source,
a memory $s(e_0)$ corresponds to leakage from the indexer expression and
$s(e_1)$ to $s(e_n)$ possible indexer expression in expression $e$. We can
also extend the leakage sources to comprise machine operations that have
operand-dependent execution latency (i.e., division in x86).

Using the leakage rules above, we can reduce proving constant-time security to
execution safety (i.e., termination without any failed assertions) by building
a \emph{self-product} for the program given the some publicly known inputs $X_i$
and construction rules below:

\begin{lstlisting}[language=MySketch]
product(p)  is  assume x = _x for x \in $X_i$;
                together(p);

together(p) is  guard(p);
                instrument($\lambda$p.(p; _p), together(p))(p)

guard(p)    is  assert L(p) = L(_p)
\end{lstlisting}

Where \texttt{\_p} denotes program \texttt{p} with renamed variables. The
product program of \texttt{p} would then be \emph{instrumented} with
assertion on the leakage from the steps it takes. The function \texttt{instrument}
does in fact recursively traverses the program statements, instruments every
branch with a gaurd that asserts the leakage so far for a program \texttt{p}, and its
renaming \texttt{\_p} is equal, then dives into the branch by evaluating
the condition using the original program.

\begin{lstlisting}[language=MySketch]
  assume in = _in;
  assume out = _out;
  assume len = _len;
  assume sub_len = _sub_len;
  i = 0; _i = 0; j = 0; _j = 0;
  assert (i < len) = (_i < _len); // trivial
  while (i < len) do:
    assert ((i $\geq$ l_idx) && (i < l_idx + sub_len)) =
               ((_i $\geq$ _l_idx) && (_i < _l_idx + _sub_len) // fails;
    if ((i $\geq$ l_idx) && (i < l_idx + sub_len)) then
        /// the rest of the program does not matter
        assert i = _i && j = _j
        out[j] = in[i]; _out[_j] = _in[_i];
        j = j + 1; _j = _j + 1;
    i = i + 1; _i = _i + 1;
\end{lstlisting}

Notice how the assertion on line 8 and 7 (which is an instrumented guards) fail,
i.e., make the product program unsafe. This example illustrates how
constant-time secutiry is reduced to assertion safety.
