\section{Reducing security to safety}

% Explain the paper, in your own words. Don't go into as many details as the
% original text, but the person reading your review should have a general
% understanding of the paper's results and how those results can be obtained.
% The structure and content of this section of course heavily depends on the
% paper itself. Don't hesitate to split it in multiple sections or subsections,
% for example: \subsection{An algorithm for whatever problem we try to solve} If
% your paper contains theorems, sketch the proofs of important theorems.

% \subsection{Benchmarks} If it contains benchmarks, show the key scores or
% results.

% You can follow the structure of the paper you're reviewing, but write with
% your own words.

% \subsection{Reducing security to safety}

At the interface of any security-sensitive function call, there usually exists a
\emph{contract} that describes which inputs or outpus are deemed public (i.e.,
known to attackers) and which ones are private (i.e., to protect). A leak should
never reveal anyting about the private inputs and outputs (but publics ones 
can be leaked).
% If we denote a program as $p$ (with the usual definition of a prgram being a
% sequence of statements or programs) with a corresponding state $s$ (which
% is mapping from variables to values) then at each step of execution, from
% state $s$ executing program/statement $p$ we can define leakage $L(.)$ as follows:

% \begin{equation}
%     \begin{split}
%     L(\langle s, \text{\texttt{if}}~e~\text{\texttt{then}}~p_1~\text{\texttt{else}}~p_2 \rangle) = & s(e) \\
%     L(\langle s, \text{\texttt{while}}~e~\text{\texttt{do}}~p \rangle) = & s(e) \\
%     L(\langle s, x_0[e_0] = e \rangle) = & s(e_0)s(e_1)...s(e_n)
%     \end{split}
% \end{equation}

% The first two leakage sources correspond to leakage from control flow while the
% latter is leakage from memory access. Note that in the third leakage source,
% a memory leak $s(e_0)$ corresponds to leakage from the right-hand-side indexer
% expression and $s(e_1)$ to $s(e_n)$ possible indexer expression in expression
% $e$. We can also extend the leakage sources to comprise machine operations that
% have operand-dependent execution latency (i.e., division in x86).

Using the leakage rules in \secref{prelimnaries}, 
we can reduce constant-time security to execution safety (i.e., termination
without any failed assertions) by building a \emph{self-product} in which two
abstract execution take place back to back, only differing in the value of
private inputs and outputs. \figref{rules} shows how a program product can
be constructed where $\hat{p}$ is the program $p$ with all variables renamed
(i.e., an alternative execution).

\begin{figure}[h]
    \centering
    \subfigure[Program product construction rules]{
        \includegraphics[width=0.45\textwidth]{./figs/fig_7.pdf}
    }\label{fig:fig_7}
    \subfigure[Instrumentation rules] {
        \includegraphics[width=0.45\textwidth]{./figs/fig_8.pdf}
    }\label{fig:fig_8}
    \caption{Program product}
    \label{fig:rules}
\end{figure}

\begin{figure}[t]
    \lstinputlisting[language=C]{example.c}
    \caption{Running example - sub-array copy}
    \label{fig:example}
  \end{figure}

The essence of the transformation is the instrumentation, which reduces 
constant-time security to assertion safety. \figref{example_prod} shows 
the product of the sub-array copy program in \figref{example}. Notice how
the program is instrumented with assertions to ensure leakage remains the same at
every step of execution. In this example, the private inputs \texttt{l\_idx} and
its renaming are not assumed to be equal, therefore the assetion on line 8 fails
and the program is proved to be unsafe and hence it follows that the program is
not constant-time secure.

\begin{figure}[h]
    \lstinputlisting[language=MySketch]{prod.my}
    \caption{example program product of the sub-array copy program.}
    \label{fig:example_prod}
\end{figure}

